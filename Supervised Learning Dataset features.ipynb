{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1431,"status":"ok","timestamp":1739581788132,"user":{"displayName":"Grant Palmer","userId":"17444665347007930116"},"user_tz":360},"id":"UMRinIaaU7G-","outputId":"2bc623c7-40ea-4b4f-9ab1-88e9477df447"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to your datasets\n","\n","training_data_path=\"/content/drive/My Drive/842975_Data.csv\" # import data\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JDcrl9H_d0q"},"outputs":[],"source":["#Indivdual Feature Evaluation\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","# Mandatory features to include\n","mandatory_features = []\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Automatically determine features by excluding the target column ('Gamma'), mandatory features, and 'Depth'\n","target_column = 'Gamma'\n","excluded_columns = mandatory_features + [target_column, 'Well', 'Depth','Hg', 'Bi','Nb','Nd', 'Pr','W','Cd','La','Sp','Ag','LE']\n","all_features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","# Target variable\n","y = training_data[target_column]\n","\n","# Initialize the results list\n","results = []\n","\n","# Evaluate each remaining feature with mandatory features\n","for feature in all_features:\n","    # Combine mandatory features with the current feature\n","    feature_set = mandatory_features + [feature]\n","    X = training_data[feature_set]\n","\n","    # Calculate the correlation between the feature set and the target ('Gamma')\n","    correlation = X.corrwith(y).iloc[-1]  # Get the correlation of the last feature (the one added in this iteration)\n","\n","    # Skip if the correlation is NaN\n","    if pd.isna(correlation):\n","        continue\n","\n","    # Calculate R-squared and Root Mean Squared Error (RMSE)\n","    from sklearn.linear_model import LinearRegression\n","    model = LinearRegression(fit_intercept=False)\n","    model.fit(X, y)\n","\n","    # Predict the values\n","    y_pred = model.predict(X)\n","\n","    # Calculate R-squared (R²)\n","    r2 = model.score(X, y)\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y, y_pred))\n","\n","    # Store the feature set, correlation, R², and RMSE\n","    equation = \" + \".join([f\"({model.coef_[i]:.4f} * {feature})\" for i, feature in enumerate(feature_set)])\n","    equation = f\"Gamma = {equation}\"\n","\n","    results.append((feature_set, correlation, r2, rmse, equation))\n","\n","# Sort results by correlation score in descending order\n","results.sort(key=lambda x: x[1], reverse=True)\n","\n","# Display results\n","print(\"\\nEquations with Each Features (Sorted by Correlation):\")\n","for idx, (feature_set, correlation, r2, rmse, equation) in enumerate(results, start=1):\n","    print(f\"\\nEquation {idx}:\")\n","    print(f\"Features: {feature_set}\")\n","    print(f\"Correlation: {correlation:.4f}\")\n","    print(f\"R-squared: {r2:.4f}\")\n","    print(f\"RMSE: {rmse:.4f}\")\n","    print(f\"{equation}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKo6_8dsz5SG"},"outputs":[],"source":["#List the features, correlation, R², MSE, and the equation to rate how useful they might be\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","\n","# Mandatory features to include\n","mandatory_features = []\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Automatically determine features by excluding the target column ('Gamma'), mandatory features, and 'Depth'\n","target_column = 'Gamma'\n","excluded_columns = mandatory_features + [target_column, 'Well', 'Depth','Hg', 'Bi','Nb','Nd', 'Pr','W','Cd','La','Sp','Ag','LE']\n","features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","# Target variable\n","y = training_data[target_column]\n","\n","# Initialize the results list\n","results = []\n","\n","# Evaluate each remaining feature with mandatory features\n","for feature in all_features:\n","    # Combine mandatory features with the current feature\n","    feature_set = mandatory_features + [feature]\n","    X = training_data[feature_set]\n","\n","    # Split the data into training and testing sets (80% training, 20% testing)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n","\n","    # Calculate the correlation between the feature set and the target ('Gamma')\n","    correlation = X_train.corrwith(y_train).iloc[-1]  # Get the correlation of the last feature (the one added in this iteration)\n","\n","    # Skip if the correlation is NaN\n","    if pd.isna(correlation):\n","        continue\n","\n","    # Train the model on the training set\n","    model = LinearRegression(fit_intercept=False)\n","    model.fit(X_train, y_train)\n","\n","    # Predict the values on the test set\n","    y_pred = model.predict(X_test)\n","\n","    # Calculate R-squared (R²) on the test set\n","    r2 = model.score(X_test, y_test)\n","\n","    # Calculate Root Mean Squared Error (MSE)\n","    mse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    # Store the feature set, correlation, R², MSE, and the equation\n","    equation = \" + \".join([f\"({model.coef_[i]:.4f} * {feature})\" for i, feature in enumerate(feature_set)])\n","    equation = f\"Gamma = {equation}\"\n","\n","    results.append((feature_set, correlation, r2, mse, equation))\n","\n","# Sort results by correlation score in descending order\n","results.sort(key=lambda x: x[1], reverse=True)\n","\n","# Display results\n","print(\"\\nEquations with Mandatory Features and One Additional Feature (Excluding 'Depth', Sorted by Correlation):\")\n","for idx, (feature_set, correlation, r2, mse, equation) in enumerate(results, start=1):\n","    print(f\"\\nEquation {idx}:\")\n","    print(f\"Features: {feature_set}\")\n","    print(f\"Correlation: {correlation:.4f}\")\n","    print(f\"R-squared: {r2:.4f}\")\n","    print(f\"MSE: {mse:.4f}\")\n","    print(f\"{equation}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gqm8drIEq7Ni"},"outputs":[],"source":["#Picks the 10 most important Features by using decision tree\n","\n","import pandas as pd\n","from sklearn.tree import DecisionTreeRegressor\n","import psutil\n","\n","# Function to monitor memory usage\n","def check_memory_limit(limit_gb):\n","    memory_info = psutil.virtual_memory()\n","    if memory_info.used > limit_gb * 1024**3:\n","        raise MemoryError(f\"Memory usage exceeded {limit_gb} GB.\")\n","\n","# Set memory limit (12GB)\n","MEMORY_LIMIT_GB = 12\n","batch_size = 10000      # Smaller batches for better memory control\n","num_processes = 30     # Reduce to limit multiprocessing overhead\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Optimize data types\n","for col in training_data.select_dtypes(include='float64').columns:\n","    training_data[col] = training_data[col].astype('float32')\n","\n","# Automatically determine features by excluding the target column ('Gamma')\n","target_column = 'Gamma'\n","excluded_columns = mandatory_features + [target_column, 'Well', 'Depth','Hg', 'Bi','Nb','Nd', 'Pr','W','Cd','La','Sp','Ag','LE']\n","features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","# Separate features (X) and target (y)\n","X = training_data[features]\n","y = training_data[target_column]\n","\n","# Standardize the features (this can help decision trees as well)\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Step 1: Fit a Decision Tree to evaluate feature importance\n","tree_model = DecisionTreeRegressor(random_state=42, max_depth=None)  # No depth limitation\n","tree_model.fit(X_scaled, y)\n","\n","# Get the feature importances from the tree model\n","feature_importances = pd.Series(tree_model.feature_importances_, index=features)\n","\n","# Step 2: Select the top 10 most important features\n","top_10_features = feature_importances.nlargest(10).index.tolist()\n","\n","# Print the top 10 features and their importance\n","print(f\"Top 10 Important Features based on Decision Tree:\\n{feature_importances.nlargest(10)}\\n\")\n","\n","\n"]},{"cell_type":"code","source":["#Creates engineerd features by multplying and dividing lists them by highest correlation\n","\n","import pandas as pd\n","from itertools import combinations\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Target column\n","target_column = 'Gamma'\n","\n","# Columns to exclude from consideration for feature engineering\n","excluded_columns = [target_column, 'Well', 'Depth', 'Hg', 'Bi', 'Nb', 'Nd', 'Pr', 'W', 'Cd', 'La', 'Sp', 'Ag', 'LE']\n","\n","# Filter the features, ensuring no zero variance\n","all_features = [\n","    col for col in training_data.columns\n","    if col not in excluded_columns and training_data[col].nunique() > 1\n","]\n","\n","# Target variable\n","y = training_data[target_column]\n","\n","# List to store results\n","results = []\n","\n","# Iterate through combinations of two features\n","for feature1, feature2 in combinations(all_features, 2):\n","    # Generate engineered features by multiplication and division\n","    engineered_features = {}\n","    engineered_features[f\"{feature1}*{feature2}\"] = training_data[feature1] * training_data[feature2]\n","    # Avoid division by zero\n","    if (training_data[feature2] != 0).all():\n","        engineered_features[f\"{feature1}/{feature2}\"] = training_data[feature1] / training_data[feature2]\n","    if (training_data[feature1] != 0).all():\n","        engineered_features[f\"{feature2}/{feature1}\"] = training_data[feature2] / training_data[feature1]\n","\n","    # Evaluate correlation with the target\n","    for feat_name, feat_values in engineered_features.items():\n","        try:\n","            correlation = feat_values.corr(y)\n","            if not pd.isna(correlation):  # Skip if correlation is NaN\n","                results.append((feat_name, correlation))\n","        except Exception as e:\n","            print(f\"Error calculating correlation for {feat_name}: {e}\")\n","\n","# Sort results by absolute value of correlation in descending order\n","results.sort(key=lambda x: abs(x[1]), reverse=True)\n","\n","# Display the top 100 results\n","if results:\n","    print(\"\\nTop 100 Engineered Features with Highest Correlation to Target:\")\n","    for idx, (feature_name, correlation) in enumerate(results[:100], start=1):\n","        print(f\"{idx}. Feature: {feature_name}, Correlation: {correlation:.4f}\")\n","else:\n","    print(\"\\nNo significant engineered features found with non-NaN correlation.\")\n","\n"],"metadata":{"id":"nM4p28rE95n_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creates engineerd features by multplying and dividing lists them by lowest correlation\n","\n","import pandas as pd\n","from itertools import combinations\n","\n","# Load the training data\n","# Adjust the path as needed\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Target column\n","target_column = 'Gamma'\n","\n","# Columns to exclude from consideration for feature engineering\n","excluded_columns = [target_column, 'Well', 'Depth', 'Hg', 'Bi', 'Nb', 'Nd', 'Pr', 'W', 'Cd', 'La', 'Sp', 'Ag', 'LE']\n","\n","# Filter the features, ensuring no zero variance\n","all_features = [\n","    col for col in training_data.columns\n","    if col not in excluded_columns and training_data[col].nunique() > 1\n","]\n","\n","# Target variable\n","y = training_data[target_column]\n","\n","# List to store results\n","results = []\n","\n","# Iterate through combinations of two features\n","for feature1, feature2 in combinations(all_features, 2):\n","    # Generate engineered features by multiplication and division\n","    engineered_features = {}\n","    engineered_features[f\"{feature1}*{feature2}\"] = training_data[feature1] * training_data[feature2]\n","    # Avoid division by zero\n","    if (training_data[feature2] != 0).all():\n","        engineered_features[f\"{feature1}/{feature2}\"] = training_data[feature1] / training_data[feature2]\n","    if (training_data[feature1] != 0).all():\n","        engineered_features[f\"{feature2}/{feature1}\"] = training_data[feature2] / training_data[feature1]\n","\n","    # Evaluate correlation with the target\n","    for feat_name, feat_values in engineered_features.items():\n","        try:\n","            correlation = feat_values.corr(y)\n","            if not pd.isna(correlation):  # Skip if correlation is NaN\n","                results.append((feat_name, correlation))\n","        except Exception as e:\n","            print(f\"Error calculating correlation for {feat_name}: {e}\")\n","\n","# Sort results by absolute value of correlation in ascending order\n","results.sort(key=lambda x: abs(x[1]))\n","\n","# Display the top 100 least correlating features\n","if results:\n","    print(\"\\nTop 100 Engineered Features with Lowest Correlation to Target:\")\n","    for idx, (feature_name, correlation) in enumerate(results[:100], start=1):\n","        print(f\"{idx}. Feature: {feature_name}, Correlation: {correlation:.4f}\")\n","else:\n","    print(\"\\nNo significant engineered features found with non-NaN correlation.\")\n"],"metadata":{"id":"0BeJePl-0_ZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8WbW2GGZG9C"},"outputs":[],"source":["#Checks every combination of feature not on the exclution list in batchs of 10000 different combinations\n","#Very Time comsuming and memory intensive\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from itertools import combinations\n","from tqdm.notebook import tqdm\n","import psutil\n","\n","# Function to monitor memory usage\n","def check_memory_limit(limit_gb):\n","    memory_info = psutil.virtual_memory()\n","    if memory_info.used > limit_gb * 1024**3:\n","        raise MemoryError(f\"Memory usage exceeded {limit_gb} GB.\")\n","\n","# Set memory limit (18GB)\n","#MEMORY_LIMIT_GB = 18\n","\n","MEMORY_LIMIT_GB = 12  # Adjust as needed\n","batch_size = 10000      # Smaller batches for better memory control\n","num_processes = 30     # Reduce to limit multiprocessing overhead\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Optimize data types\n","for col in training_data.select_dtypes(include='float64').columns:\n","    training_data[col] = training_data[col].astype('float32')\n","\n","# Automatically determine features by excluding the target column ('Gamma')\n","target_column = 'Gamma'\n","excluded_columns = mandatory_features + [target_column, 'Well', 'Depth','Hg', 'Bi','Nb','Nd', 'Pr','W','Cd','La','Sp','Ag','LE']\n","features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","\n","# Model to evaluate\n","model = LinearRegression()\n","\n","# Initialize a list to store the top equations and their R2 scores\n","top_equations = []\n","\n","# Generate and evaluate combinations in batches\n","batch_size = 10000  # Number of feature sets per batch\n","combination_generator = (\n","    combo for r in range(1, len(features) + 1) for combo in combinations(features, r)\n",")\n","\n","# Batch processing with memory checks\n","batch = []\n","for feature_set in tqdm(combination_generator, desc=\"Evaluating feature sets\"):\n","    batch.append(feature_set)\n","    if len(batch) >= batch_size:\n","        # Check memory usage\n","        check_memory_limit(MEMORY_LIMIT_GB)\n","\n","        # Process the batch\n","        for feature_set in batch:\n","            X = training_data[list(feature_set)]\n","            y = training_data[target_column]\n","\n","            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n","\n","            model.fit(X_train, y_train)\n","            predictions = model.predict(X_test)\n","            r2 = r2_score(y_test, predictions)\n","\n","            # Store the feature set and its R2 score\n","            top_equations.append((feature_set, r2))\n","\n","        # Clear the batch\n","        batch = []\n","\n","# Process remaining feature sets in the batch\n","if batch:\n","    for feature_set in batch:\n","        X = training_data[list(feature_set)]\n","        y = training_data[target_column]\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n","\n","        model.fit(X_train, y_train)\n","        predictions = model.predict(X_test)\n","        r2 = r2_score(y_test, predictions)\n","\n","        # Store the feature set and its R2 score\n","        top_equations.append((feature_set, r2))\n","\n","# Sort the top equations by R2 score\n","top_equations.sort(key=lambda x: x[1], reverse=True)\n","\n","# Display the top 100 equations with their R2 scores\n","print(\"\\nTop 100 Equations with R2 Scores:\")\n","for idx, (feature_set, r2) in enumerate(top_equations[:100], start=1):\n","    equation = \" + \".join([f\"({model.coef_[j]:.4f} * {feature})\" for j, feature in enumerate(feature_set)])\n","    intercept = f\"{model.intercept_:.4f}\"\n","    print(f\"Equation {idx}: R2 Score = {r2:.4f}\")\n","    print(f\"Gamma = {equation} + {intercept}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfEFIwqBU-uz"},"outputs":[],"source":["\"\"\"Compares the Mandatory features plus 1 additional feature and produces a linear equasion with no intercept\n","add features you want to include into the mandatory_features list\n","add features you want to exclude from the all_features list\"\"\"\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","\n","# Mandatory features to include\n","mandatory_features = []\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Optimize data types\n","for col in training_data.select_dtypes(include='float64').columns:\n","    training_data[col] = training_data[col].astype('float32')\n","\n","# Automatically determine features by excluding the target column ('Gamma'), mandatory features, and 'Depth'\n","target_column = 'Gamma'\n","excluded_columns = mandatory_features + [target_column, 'Well', 'Depth','Hg', 'Bi','Nb','Nd', 'Pr','W','Cd','La','Sp','Ag','LE']\n","all_features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","# Initialize the model\n","model = LinearRegression()\n","\n","# Target variable\n","y = training_data[target_column]\n","\n","# Evaluate each remaining feature with mandatory features\n","results = []\n","\n","for feature in all_features:\n","    # Combine mandatory features with the current feature\n","    feature_set = mandatory_features + [feature]\n","    X = training_data[feature_set]\n","\n","    # Split the data\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n","\n","    # Fit the model\n","    model = LinearRegression(fit_intercept=False)\n","    model.fit(X_train, y_train)\n","    predictions = model.predict(X_test)\n","    r2 = r2_score(y_test, predictions)\n","\n","    # Store the feature set, equation, and R2 score\n","    coefficients = model.coef_\n","    intercept = model.intercept_\n","    equation = \" + \".join([f\"({coefficients[i]:.4f} * {feature})\" for i, feature in enumerate(feature_set)])\n","    equation = f\"Gamma = {equation}\"\n","    results.append((feature_set, equation, r2))\n","\n","# Sort results by R2 score\n","results.sort(key=lambda x: x[2], reverse=True)\n","\n","# Display results\n","print(\"\\nEquations with Mandatory Features and One Additional Feature (Sorted by R2):\")\n","for idx, (feature_set, equation, r2) in enumerate(results, start=1):\n","    print(f\"\\nEquation {idx}:\")\n","    print(f\"Features: {feature_set}\")\n","    print(f\"R2 Score: {r2:.4f}\")\n","    print(f\"{equation}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQF3or4bz5NR"},"outputs":[],"source":["\"\"\"Compares the Mandatory features plus a set number of features and produces a linear equasion with an intercept\n","\n","running combinations of more than 4 can greatly increase run time\n","\n","add features you want to include into the mandatory_features list\n","add features you want to exclude from the all_features list\"\"\"\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from itertools import combinations\n","\n","# Mandatory features to include\n","mandatory_features = []\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Optimize data types\n","for col in training_data.select_dtypes(include='float64').columns:\n","    training_data[col] = training_data[col].astype('float32')\n","\n","# Automatically determine features by excluding the target column ('Gamma'), mandatory features, and 'Depth'\n","target_column = 'Gamma'\n","excluded_columns = mandatory_features + [target_column, 'Well', 'Depth','Hg', 'Bi','Nb','Nd', 'Pr','W','Cd','La','Sp','Ag','LE']\n","features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","# Initialize the model with an intercept\n","model = LinearRegression(fit_intercept=True)\n","\n","# Target variable\n","y = training_data[target_column]\n","\n","# Evaluate each pair of remaining features with mandatory features\n","results = []\n","\n","#Enter the number of feature combinations to check\n","  #more than 4 can greatly increase run time\n","for feature_pair in combinations(all_features, 4):\n","    # Combine mandatory features with the current feature pair\n","    feature_set = mandatory_features + list(feature_pair)\n","    X = training_data[feature_set]\n","\n","    # Split the data\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n","\n","    # Fit the model\n","    model.fit(X_train, y_train)\n","    predictions = model.predict(X_test)\n","    r2 = r2_score(y_test, predictions)\n","\n","    # Store the feature set, equation, and R2 score\n","    coefficients = model.coef_\n","    equation = \" + \".join([f\"({coefficients[i]:.4f} * {feature})\" for i, feature in enumerate(feature_set)])\n","    equation += f\" + ({model.intercept_:.4f})\"\n","    results.append((feature_set, equation, r2))\n","\n","# Sort results by R2 score\n","results.sort(key=lambda x: x[2], reverse=True)\n","\n","# Display results\n","print(\"\\nEquations with Mandatory Features and Additional Features (With Intercept, Sorted by R2):\")\n","for idx, (feature_set, equation, r2) in enumerate(results, start=1):\n","    print(f\"\\nEquation {idx}:\")\n","    print(f\"Features: {feature_set}\")\n","    print(f\"R2 Score: {r2:.4f}\")\n","    print(f\"{equation}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCN5aakMz5GS"},"outputs":[],"source":["\"\"\"Compares the Mandatory features plus a set number of features and produces a linear equasion with no intercept\n","\n","running combinations of more than 4 can greatly increase run time\n","\n","add features you want to include into the mandatory_features list\n","add features you want to exclude from the all_features list\"\"\"\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from itertools import combinations\n","\n","# Mandatory features to include\n","mandatory_features = []\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Optimize data types\n","for col in training_data.select_dtypes(include='float64').columns:\n","    training_data[col] = training_data[col].astype('float32')\n","\n","# Automatically determine features by excluding the target column ('Gamma'), mandatory features, and 'Depth'\n","target_column = 'Gamma'\n","excluded_columns = mandatory_features + [target_column, 'Well', 'Depth','Hg', 'Bi','Nb','Nd', 'Pr','W','Cd','La','Sp','Ag','LE']\n","features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","# Initialize the model without an intercept\n","model = LinearRegression(fit_intercept=False)\n","\n","# Evaluate each pair of remaining features with mandatory features\n","results = []\n","\n","\n","#Enter the number of feature combinations to check\n","  #more than 4 can greatly increase run time\n","for feature_pair in combinations(all_features, 4):\n","    # Combine mandatory features with the current feature pair\n","    feature_set = mandatory_features + list(feature_pair)\n","    X = training_data[feature_set]\n","\n","    # Split the data\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n","\n","    # Fit the model\n","    model.fit(X_train, y_train)\n","    predictions = model.predict(X_test)\n","    r2 = r2_score(y_test, predictions)\n","\n","    # Store the feature set, equation, and R2 score\n","    coefficients = model.coef_\n","    equation = \" + \".join([f\"({coefficients[i]:.4f} * {feature})\" for i, feature in enumerate(feature_set)])\n","    equation = f\"Gamma = {equation}\"\n","    results.append((feature_set, equation, r2))\n","\n","# Sort results by R2 score\n","results.sort(key=lambda x: x[2], reverse=True)\n","\n","# Display results\n","print(\"\\nEquations with Mandatory Features and Additional Features (No Intercept, Sorted by R2):\")\n","for idx, (feature_set, equation, r2) in enumerate(results, start=1):\n","    print(f\"\\nEquation {idx}:\")\n","    print(f\"Features: {feature_set}\")\n","    print(f\"R2 Score: {r2:.4f}\")\n","    print(f\"{equation}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08_PAphmf_Dp"},"outputs":[],"source":["\"\"\"Compares the Mandatory features plus a set number of features and produces a linear equasion with no intercept\n","Requested Overfitting model that uses the 100% of the dataset as training data\n","\n","running combinations of more than 4 can greatly increase run time\n","\n","add features you want to include into the mandatory_features list\n","add features you want to exclude from the all_features list\"\"\"\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from itertools import combinations\n","\n","# Mandatory features to include\n","mandatory_features = []\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Optimize data types\n","for col in training_data.select_dtypes(include='float64').columns:\n","    training_data[col] = training_data[col].astype('float32')\n","\n","# Automatically determine features by excluding the target column ('Gamma'), mandatory features, and 'Depth'\n","target_column = 'Gamma'\n","excluded_columns = mandatory_features + [target_column, 'Well', 'Depth', 'Hg', 'Bi', 'Nb', 'Nd', 'Pr', 'W', 'Cd', 'La', 'Sp', 'Ag', 'LE']\n","all_features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","# Initialize the model without an intercept\n","model = LinearRegression(fit_intercept=False)\n","\n","# Evaluate each triplet of remaining features with mandatory features\n","results = []\n","\n","for feature_triplet in combinations(all_features, 4):\n","    # Combine mandatory features with the current feature triplet\n","    feature_set = mandatory_features + list(feature_triplet)\n","\n","    # Select features and target\n","    X = training_data[feature_set]\n","    y = training_data[target_column]\n","\n","    # Drop rows with NaN values to ensure consistency\n","    combined = pd.concat([X, y], axis=1).dropna()\n","    X = combined[feature_set]\n","    y = combined[target_column]\n","\n","    # Fit the model on the entire dataset\n","    model.fit(X, y)\n","    predictions = model.predict(X)\n","    r2 = r2_score(y, predictions)\n","\n","    # Store the feature set, equation, and R2 score\n","    coefficients = model.coef_\n","    equation = \" + \".join([f\"({coefficients[i]:.5f} * {feature})\" for i, feature in enumerate(feature_set)])\n","    equation = f\"Gamma = {equation}\"\n","    results.append((feature_set, equation, r2))\n","\n","# Sort results by R2 score\n","results.sort(key=lambda x: x[2], reverse=True)\n","\n","# Display only the top 100 results\n","print(\"\\nTop 100 Equations with Mandatory Features and Additional Features (With Intercept, Sorted by R2):\")\n","for idx, (feature_set, equation, r2) in enumerate(results[:100], start=1):  # Slice the top 100 results\n","    print(f\"\\nEquation {idx}:\")\n","    print(f\"Features: {feature_set}\")\n","    print(f\"R2 Score: {r2:.4f}\")\n","    print(f\"{equation}\")\n"]},{"cell_type":"code","source":["\"\"\"Compares the features in all combinations and produces a linear equasion with an intercept\n","it checkes in bacthes to handle large datasets without crashing\n","\n","For datasets with many features it can take a long time to run\n","\n","add features you want to exclude from the all_features list\"\"\"\n","\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from itertools import combinations\n","from tqdm.notebook import tqdm\n","import psutil\n","\n","# Function to monitor memory usage\n","def check_memory_limit(limit_gb):\n","    memory_info = psutil.virtual_memory()\n","    if memory_info.used > limit_gb * 1024**3:\n","        raise MemoryError(f\"Memory usage exceeded {limit_gb} GB.\")\n","\n","# Set memory limit (18GB)\n","MEMORY_LIMIT_GB = 18\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Optimize data types\n","for col in training_data.select_dtypes(include='float64').columns:\n","    training_data[col] = training_data[col].astype('float32')\n","\n","# Automatically determine features by excluding the target column ('Gamma')\n","target_column = 'Gamma'\n","excluded_columns = [target_column, 'Well', 'Depth', 'Hg', 'Bi', 'Nb', 'Nd', 'Pr', 'W', 'Cd', 'La', 'Sp', 'Ag', 'LE']\n","features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","# Model to evaluate\n","model = LinearRegression()\n","\n","# Initialize a list to store the top equations and their R2 scores\n","top_equations = []\n","\n","# Generate and evaluate combinations in batches\n","batch_size = 1000  # Number of feature sets per batch\n","combination_generator = (\n","    combo for r in range(1, len(features) + 1) for combo in combinations(features, r)\n",")\n","\n","# Batch processing with memory checks\n","batch = []\n","for feature_set in tqdm(combination_generator, desc=\"Evaluating feature sets\"):\n","    batch.append(feature_set)\n","    if len(batch) >= batch_size:\n","        # Check memory usage\n","        check_memory_limit(MEMORY_LIMIT_GB)\n","\n","        # Process the batch\n","        for feature_set in batch:\n","            X = training_data[list(feature_set)]\n","            y = training_data[target_column]\n","\n","            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n","\n","            model.fit(X_train, y_train)\n","            predictions = model.predict(X_test)\n","            r2 = r2_score(y_test, predictions)\n","\n","            # Store the feature set and its R2 score\n","            top_equations.append((feature_set, r2))\n","\n","        # Clear the batch\n","        batch = []\n","\n","# Process remaining feature sets in the batch\n","if batch:\n","    for feature_set in batch:\n","        X = training_data[list(feature_set)]\n","        y = training_data[target_column]\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n","\n","        model.fit(X_train, y_train)\n","        predictions = model.predict(X_test)\n","        r2 = r2_score(y_test, predictions)\n","\n","        # Store the feature set and its R2 score\n","        top_equations.append((feature_set, r2))\n","\n","# Sort the top equations by R2 score\n","top_equations.sort(key=lambda x: x[1], reverse=True)\n","\n","# Display the top 100 equations with their R2 scores\n","print(\"\\nTop 100 Equations with R2 Scores:\")\n","for idx, (feature_set, r2) in enumerate(top_equations[:100], start=1):\n","    equation = \" + \".join([f\"({model.coef_[j]:.4f} * {feature})\" for j, feature in enumerate(feature_set)])\n","    intercept = f\"{model.intercept_:.4f}\"\n","    print(f\"Equation {idx}: R2 Score = {r2:.4f}\")\n","    print(f\"Gamma = {equation} + {intercept}\\n\")"],"metadata":{"id":"g1cb4tI1jhxD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pK3mVWCpc7Ln"},"outputs":[],"source":["\"\"\"Runs through the set number of itterations and produces a linear equasion with an intercept\n","\n","Different randam seeds will produce different results\n","\n","add features you want to exclude from the all_features list\"\"\"\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.metrics import r2_score\n","from sklearn.preprocessing import StandardScaler\n","import psutil\n","from tqdm.notebook import tqdm\n","\n","\n","# Load the training data\n","training_data = pd.read_csv(training_data_path, na_values=['#NUM!', '#DIV/0!'])\n","\n","# Drop rows with NaN values\n","training_data.dropna(inplace=True)\n","\n","# Remove 'Depth' from features\n","target_column = 'Gamma'\n","excluded_columns = [target_column, 'Well', 'Depth', 'Hg', 'Bi', 'Nb', 'Nd', 'Pr', 'W', 'Cd', 'La', 'Sp', 'Ag', 'LE']\n","features = [col for col in training_data.columns if col not in excluded_columns]\n","\n","\n","# Optimize data types\n","for col in training_data.select_dtypes(include='float64').columns:\n","    training_data[col] = training_data[col].astype('float32')\n","\n","# Model to evaluate using SGD\n","model = SGDRegressor(max_iter=100000, tol=1e-3, random_state=42)\n","\n","# Initialize a list to store feature sets and their R2 scores\n","top_equations = []\n","\n","# Batch processing with memory checks\n","for batch_start in tqdm(range(0, len(features), batch_size), desc=\"Evaluating feature sets\"):\n","    batch_end = min(batch_start + batch_size, len(features))\n","    batch_features = features[batch_start:batch_end]\n","\n","    # Extract data for the current batch of features\n","    X = training_data[batch_features]\n","    y = training_data[target_column]\n","\n","    # Scale the features to improve the model performance\n","    scaler = StandardScaler()\n","    X_scaled = scaler.fit_transform(X)\n","\n","    # Split into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n","\n","    # Train the model using SGD\n","    model.fit(X_train, y_train)\n","\n","    # Make predictions and evaluate the model\n","    predictions = model.predict(X_test)\n","    r2 = r2_score(y_test, predictions)\n","\n","    # Store the feature set and its R2 score\n","    top_equations.append((batch_features, r2))\n","\n","    # Check memory usage during batch processing\n","    check_memory_limit(MEMORY_LIMIT_GB)\n","\n","# Sort the equations by R2 score in descending order\n","top_equations.sort(key=lambda x: x[1], reverse=True)\n","\n","# Display the top 10 feature sets with their R2 scores\n","print(\"\\nEquision with R2 Score:\")\n","for idx, (feature_set, r2) in enumerate(top_equations[:1], start=1):\n","    equation = \" + \".join([f\"({model.coef_[i]:.4f} * {feature})\" for i, feature in enumerate(feature_set)])\n","    intercept = f\"{model.intercept_[0]:.4f}\"  # Access the scalar value of the intercept\n","    print(f\"Equation {idx}: R2 Score = {r2:.4f}\")\n","    print(f\"Gamma = {equation} + {intercept}\\n\")\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfwf8Cbiac0DS1pw0ctQBm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}